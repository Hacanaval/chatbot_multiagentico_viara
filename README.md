# ğŸ¤– Chatbot Multiagente VIARA
### *Asistente de Marketing Digital con IA Local*

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-green.svg)](https://ollama.ai)
[![Streamlit](https://img.shields.io/badge/Streamlit-UI-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ DescripciÃ³n

**VIARA** es un chatbot multiagente modular diseÃ±ado especÃ­ficamente para agencias de marketing digital. Funciona **completamente local** usando modelos LLM de Ollama, garantizando privacidad total de los datos de tus clientes.

### ğŸ¯ **Â¿Para quÃ© sirve?**

- **GeneraciÃ³n de mallas de contenido** personalizadas por cliente
- **Estrategias de marketing digital** adaptadas a cada marca
- **Copys y contenido** optimizado para redes sociales
- **Flujo de trabajo profesional** con revisiÃ³n y pulido automÃ¡tico

## ğŸ—ï¸ Arquitectura del Sistema

```
[Consulta Usuario] 
    â†“
[Orquestador Paralelo]
    â”œâ”€â”€ ğŸ§  Agente Estrategia (Genera ideas y planificaciÃ³n)
    â”œâ”€â”€ ğŸ¨ Agente Contexto (Adapta al cliente especÃ­fico)  
    â””â”€â”€ âœ¨ Agente CrÃ­tico (Revisa y pule el resultado)
    â†“
[Respuesta Final Optimizada]
```

### **Flujo Multiagente:**
1. **Estrategia**: Procesa la consulta usando las metodologÃ­as de tu agencia
2. **Contexto**: Adapta el contenido al tono y caracterÃ­sticas del cliente especÃ­fico
3. **CrÃ­tico**: Revisa, mejora y garantiza calidad humana (no AI-detectable)

## ğŸš€ InstalaciÃ³n RÃ¡pida

### **Prerrequisitos**
- Python 3.10+
- [Ollama](https://ollama.ai) instalado localmente

### **1. Clonar el repositorio**
```bash
git clone https://github.com/Hacanaval/chatbot_multiagentico_viara.git
cd chatbot_multiagentico_viara
```

### **2. Instalar dependencias**
```bash
pip install -r requirements.txt
```

### **3. Configurar Ollama**
```bash
# Descargar modelo recomendado
ollama pull phi3:mini

# Verificar que funciona
ollama run phi3:mini "Hola, Â¿funciona correctamente?"
```

### **4. Probar el sistema**
```bash
python orchestrator_parallel.py
```

## ğŸ“ Estructura del Proyecto

```
chatbot_multiagentico_viara/
â”‚
â”œâ”€â”€ ğŸ¯ orchestrator_parallel.py    # Orquestador principal (recomendado)
â”œâ”€â”€ ğŸ”„ orchestrator_autogen.py     # Alternativa con AutoGen
â”‚
â”œâ”€â”€ agents/                        # ğŸ¤– LÃ³gica de cada agente
â”‚   â”œâ”€â”€ estrategia.py              # Genera estrategias de contenido
â”‚   â”œâ”€â”€ contexto.py                # Adapta al cliente especÃ­fico
â”‚   â””â”€â”€ critico.py                 # Revisa y pule resultados
â”‚
â”œâ”€â”€ prompts/                       # ğŸ“ Prompts modulares editables
â”‚   â”œâ”€â”€ estrategia.md              # Template del agente estratÃ©gico
â”‚   â”œâ”€â”€ contexto.md                # Template del adaptador de contexto
â”‚   â””â”€â”€ critico.md                 # Template del revisor crÃ­tico
â”‚
â”œâ”€â”€ data/                          # ğŸ“Š Datos de configuraciÃ³n
â”‚   â”œâ”€â”€ instrucciones.json         # MetodologÃ­a de tu agencia
â”‚   â””â”€â”€ clientes/                  # Configuraciones por cliente
â”‚       â””â”€â”€ everest_cocktails.json # Ejemplo: cliente de cocteles
â”‚
â”œâ”€â”€ utils/                         # ğŸ› ï¸ Funciones auxiliares
â”‚   â”œâ”€â”€ ollama_client.py           # Cliente para LLM local
â”‚   â”œâ”€â”€ loader.py                  # Carga archivos JSON/MD
â”‚   â””â”€â”€ llm_config_ollama.py       # Config para AutoGen
â”‚
â”œâ”€â”€ memory/                        # ğŸ’¾ Memoria conversacional (v2.0)
â”‚   â””â”€â”€ chat_logs/                 # Historial futuro
â”‚
â””â”€â”€ requirements.txt               # Dependencias del proyecto
```

## ğŸ® Uso BÃ¡sico

### **EjecuciÃ³n Directa**
```python
from orchestrator_parallel import run_pipeline_parallel

resultado = run_pipeline_parallel(
    consulta_usuario="Genera una malla de contenido para septiembre",
    cliente_file="everest_cocktails.json"
)
print(resultado)
```

### **Personalizar Cliente**
1. Copia `data/clientes/everest_cocktails.json`
2. Edita los datos de tu cliente:
```json
{
  "empresa": "Tu Cliente",
  "sector": "Industria especÃ­fica",
  "menciones_estrategicas": {
    "tono": "profesional y cercano",
    "hashtags_frecuentes": ["#TuMarca", "#Industria"]
  }
}
```

### **Ajustar MetodologÃ­a**
Edita `data/instrucciones.json` con tus procesos internos:
```json
{
  "tono": "Tu tono caracterÃ­stico",
  "frecuencia_publicacion": "Frecuencia deseada",
  "metodologia": ["Paso 1", "Paso 2", "..."]
}
```

## ğŸ¨ PersonalizaciÃ³n de Prompts

Los prompts estÃ¡n externalizados en `prompts/*.md` para fÃ¡cil ediciÃ³n:

- **`estrategia.md`**: Define cÃ³mo generar estrategias
- **`contexto.md`**: Controla la adaptaciÃ³n por cliente  
- **`critico.md`**: Establece criterios de calidad

**Ejemplo de personalizaciÃ³n:**
```markdown
# prompts/estrategia.md
Eres el **Planner EstratÃ©gico** de [TU AGENCIA].

### Instrucciones especÃ­ficas:
- Siempre incluir 3 pilares de contenido
- MÃ­nimo 10 ideas por malla
- Formato: Carrusel > Reel > Historia
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### **Cambiar Modelo LLM**
```python
# En utils/ollama_client.py
def run_llm(prompt: str, model: str = "llama3:8b"):  # Cambiar aquÃ­
```

### **Ajustar Temperatura**
```python
# En agents/critico.py
return run_llm(prompt, temperature=0.1)  # MÃ¡s conservador
```

### **Orquestador AutoGen** (Alternativo)
```python
from orchestrator_autogen import run_pipeline_autogen

resultado = run_pipeline_autogen(
    consulta_usuario="Tu consulta",
    cliente_file="cliente.json"
)
```

## ğŸ§ª Testing

```bash
# Test bÃ¡sico del orquestador
python orchestrator_parallel.py

# Test con cliente especÃ­fico
python -c "from orchestrator_parallel import run_pipeline_parallel; print(run_pipeline_parallel('Test', 'everest_cocktails.json'))"
```

## ğŸ›£ï¸ Roadmap

### **v1.0 (Actual) âœ…**
- [x] Arquitectura multiagente funcional
- [x] Prompts modulares editables
- [x] ConfiguraciÃ³n por cliente
- [x] OrquestaciÃ³n paralela optimizada

### **v2.0 (PrÃ³ximo) ğŸš§**
- [ ] Memoria conversacional persistente
- [ ] Interfaz web con Streamlit
- [ ] MÃ©tricas de calidad automÃ¡ticas
- [ ] Templates de mallas predefinidos

### **v3.0 (Futuro) ğŸ”®**
- [ ] IntegraciÃ³n con APIs de RRSS
- [ ] AnÃ¡lisis de competencia automÃ¡tico
- [ ] GeneraciÃ³n de imÃ¡genes con AI
- [ ] Dashboard de rendimiento

## ğŸ¤ Contribuir

Â¿Quieres mejorar VIARA? Â¡Contributions son bienvenidas!

1. Fork del repositorio
2. Crea una branch: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m 'AÃ±ade nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver [LICENSE](LICENSE) para detalles.

## ğŸ†˜ Soporte

Â¿Problemas o preguntas?

- ğŸ› **Issues**: [GitHub Issues](https://github.com/Hacanaval/chatbot_multiagentico_viara/issues)
- ğŸ“§ **Email**: [tu-email@ejemplo.com]
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/Hacanaval/chatbot_multiagentico_viara/discussions)

## ğŸ† CrÃ©ditos

Desarrollado con â¤ï¸ para agencias de marketing digital que valoran:
- **Privacidad** (100% local)
- **PersonalizaciÃ³n** (adaptable a tu metodologÃ­a)
- **Calidad** (output indistinguible de humano)
- **Eficiencia** (automatizaciÃ³n inteligente)

---

### ğŸŒŸ **Â¿Te gusta el proyecto? Â¡Dale una estrella!** â­

![VIARA Logo](https://via.placeholder.com/800x200/4CAF50/white?text=VIARA+%7C+Marketing+AI+Local)